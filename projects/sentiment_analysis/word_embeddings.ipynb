{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An alternative approach\n",
    "Can we define words by the company they keep?  \n",
    "\"If A and B have almost the identical environments we can say that they are synonyms\" (Zelig Harris, 1954)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector representations\n",
    "One-hot encodings are long and sparse  \n",
    "Alternative: **dense vectors**  \n",
    "short (length 50-1000) + dense (most elements are non-zero)\n",
    "### Benefits\n",
    "1. Easier to use in ML  \n",
    "2. Offer better generalization capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train your own embeddings\n",
    "Instead of counting terms, we train a classifier on a prediction task: 'does A occur near B'?  \n",
    "The learned classifier weights become our embeddings   \n",
    "We can create our own word embeddings using gensim (an open-source library for unsupervised topic modeling and NLP)  \n",
    "and train it on the Brown corpus  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:54:42.480781Z",
     "start_time": "2025-12-06T06:54:40.483594Z"
    }
   },
   "source": [
    "# set up libraries & data\n",
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "nltk.download('brown')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/atsushihatakeyama/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:54:49.336354Z",
     "start_time": "2025-12-06T06:54:42.645155Z"
    }
   },
   "source": [
    "# train the model\n",
    "model = gensim.models.Word2Vec(brown.sents())"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:54:49.361917Z",
     "start_time": "2025-12-06T06:54:49.349465Z"
    }
   },
   "source": [
    "# save a copy, for later re-use\n",
    "model.save('brown.embedding')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:54:49.443876Z",
     "start_time": "2025-12-06T06:54:49.381553Z"
    }
   },
   "source": [
    "# we can load models on demand\n",
    "brown_model = gensim.models.Word2Vec.load('brown.embedding')"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:54:49.451774Z",
     "start_time": "2025-12-06T06:54:49.449641Z"
    }
   },
   "source": [
    "# how many words?\n",
    "len(brown_model.wv.index_to_key)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15173"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:54:49.526944Z",
     "start_time": "2025-12-06T06:54:49.521657Z"
    }
   },
   "source": [
    "# how many dimensions?\n",
    "brown_model.wv['university']"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10108075,  0.22742166,  0.3202568 ,  0.23411836, -0.21170661,\n",
       "       -0.30038887,  0.409905  ,  0.35845628, -0.30588168, -0.21326882,\n",
       "       -0.14555615, -0.25232542,  0.0048379 ,  0.16435574,  0.41183546,\n",
       "       -0.15681866,  0.31765732, -0.06036779, -0.3866692 , -0.6257796 ,\n",
       "        0.32944852, -0.1516606 ,  0.42890576,  0.07599909, -0.01374124,\n",
       "       -0.10768668, -0.232468  ,  0.09907427, -0.38865626,  0.1297758 ,\n",
       "        0.27606502,  0.02651284,  0.12728298, -0.24867278,  0.00420449,\n",
       "        0.02078652, -0.15709856,  0.10768691, -0.21103078, -0.04226524,\n",
       "       -0.02831067, -0.23004474,  0.0896466 ,  0.07167623,  0.0951155 ,\n",
       "       -0.06976258, -0.09656906, -0.12872428,  0.05893157,  0.25232992,\n",
       "        0.24184497, -0.3139338 , -0.17986003, -0.02292724, -0.1581031 ,\n",
       "       -0.16923527,  0.26761666,  0.06093049, -0.19247353, -0.13254713,\n",
       "        0.04970529,  0.12182314,  0.11722158, -0.09273087, -0.33360812,\n",
       "        0.23861901,  0.03607434,  0.18643394, -0.09065596,  0.33307755,\n",
       "        0.10959972,  0.28676665,  0.19665575, -0.00272151,  0.39873025,\n",
       "        0.14319351,  0.31520975,  0.06445356,  0.11258533, -0.31170171,\n",
       "       -0.3384206 , -0.06328041, -0.19404233,  0.07964049, -0.17568974,\n",
       "       -0.04243176,  0.31493324,  0.07574541,  0.11652941,  0.30474174,\n",
       "        0.29861245, -0.02568742, -0.01764435, -0.04236524,  0.17434819,\n",
       "        0.06831805,  0.25452384, -0.3685047 , -0.30188987, -0.14004847],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:54:49.566242Z",
     "start_time": "2025-12-06T06:54:49.562574Z"
    }
   },
   "source": [
    "# calculate similarity between terms\n",
    "brown_model.wv.similarity('university','inception')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.822145)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:54:49.604291Z",
     "start_time": "2025-12-06T06:54:49.588599Z"
    }
   },
   "source": [
    "# find similar terms\n",
    "brown_model.wv.most_similar('university', topn=5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('membership', 0.9555637836456299),\n",
       " ('inception', 0.9551917910575867),\n",
       " ('profession', 0.9533959627151489),\n",
       " ('neighborhood', 0.951281726360321),\n",
       " ('congregation', 0.950244665145874)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:54:49.622169Z",
     "start_time": "2025-12-06T06:54:49.618273Z"
    }
   },
   "source": [
    "brown_model.wv.most_similar('lemon', topn=5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marble', 0.9649989008903503),\n",
       " ('elaborate', 0.9647454023361206),\n",
       " ('neat', 0.9607310891151428),\n",
       " ('Cape', 0.9601399302482605),\n",
       " ('pension', 0.9597789645195007)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:54:49.638308Z",
     "start_time": "2025-12-06T06:54:49.635080Z"
    }
   },
   "source": [
    "brown_model.wv.most_similar('government', topn=5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('power', 0.9291643500328064),\n",
       " ('policy', 0.927480161190033),\n",
       " ('education', 0.9245082139968872),\n",
       " ('Christian', 0.920354425907135),\n",
       " ('nation', 0.9192814826965332)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use pre-trained embeddings\n",
    "We can load pre-built embeddings, e.g. a sample from a model trained on 100 billion words from the Google News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:22.837521Z",
     "start_time": "2025-12-06T06:54:49.664840Z"
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('word2vec_sample')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /Users/atsushihatakeyama/nltk_data...\n",
      "[nltk_data]   Unzipping models/word2vec_sample.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:31.920086Z",
     "start_time": "2025-12-06T06:56:22.931515Z"
    }
   },
   "source": [
    "# load a pre-build model\n",
    "from nltk.data import find\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "news_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:31.970722Z",
     "start_time": "2025-12-06T06:56:31.964004Z"
    }
   },
   "source": [
    "# how many terms?\n",
    "len(news_model.key_to_index)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43981"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.107503Z",
     "start_time": "2025-12-06T06:56:32.104197Z"
    }
   },
   "source": [
    "# how many dimensions?\n",
    "len(news_model['university'])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.314902Z",
     "start_time": "2025-12-06T06:56:32.238813Z"
    }
   },
   "source": [
    "# are they any better?\n",
    "news_model.most_similar(positive=['university'], topn = 5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003918290138245),\n",
       " ('faculty', 0.6780906915664673),\n",
       " ('undergraduate', 0.6587096452713013),\n",
       " ('campus', 0.6434987783432007),\n",
       " ('college', 0.6385269165039062)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.354819Z",
     "start_time": "2025-12-06T06:56:32.335277Z"
    }
   },
   "source": [
    "news_model.most_similar(positive=['lemon'], topn = 5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lemons', 0.646256148815155),\n",
       " ('apricot', 0.619941771030426),\n",
       " ('avocado', 0.5922888517379761),\n",
       " ('fennel', 0.5873182415962219),\n",
       " ('coriander', 0.5828487277030945)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.378354Z",
     "start_time": "2025-12-06T06:56:32.370286Z"
    }
   },
   "source": [
    "news_model.most_similar(positive=['government'], topn = 5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Government', 0.7132059931755066),\n",
       " ('governments', 0.6521531939506531),\n",
       " ('administration', 0.5462369322776794),\n",
       " ('legislature', 0.5307288765907288),\n",
       " ('parliament', 0.5268454551696777)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform vector algebra\n",
    "We can use embeddings to perform verbal reasoning, e.g. A is to B as C is to...  \n",
    "e.g. 'man is to king as woman is to...'  \n",
    "vec(“king”) - vec(“man”) + vec(“woman”) =~ vec(“queen”)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.418474Z",
     "start_time": "2025-12-06T06:56:32.409393Z"
    }
   },
   "source": [
    "news_model.most_similar(positive=['woman','king'], negative=['man'], topn = 5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236843228340149)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.472255Z",
     "start_time": "2025-12-06T06:56:32.447700Z"
    }
   },
   "source": [
    "news_model.most_similar(positive=['king', 'woman'], negative=['man'], topn = 5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236843228340149)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.522753Z",
     "start_time": "2025-12-06T06:56:32.508164Z"
    }
   },
   "source": [
    "# encyclopaedic knowledge\n",
    "news_model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 0.7884091734886169),\n",
       " ('Belgium', 0.6197876930236816),\n",
       " ('Spain', 0.566477358341217),\n",
       " ('Italy', 0.5654898881912231),\n",
       " ('Switzerland', 0.560969352722168)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.567819Z",
     "start_time": "2025-12-06T06:56:32.554203Z"
    }
   },
   "source": [
    "# syntactic patterns (verbs)\n",
    "news_model.most_similar(positive=['has','be'], negative=['have'], topn = 5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 0.6774995923042297),\n",
       " ('was', 0.5710029006004333),\n",
       " ('remains', 0.47552669048309326),\n",
       " ('been', 0.4538104236125946),\n",
       " ('being', 0.4456518888473511)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.603102Z",
     "start_time": "2025-12-06T06:56:32.592627Z"
    }
   },
   "source": [
    "# syntactic patterns (adjectives)\n",
    "news_model.most_similar(positive=['longest','short'], negative=['long'], topn = 5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shortest', 0.5145130753517151),\n",
       " ('steepest', 0.42448344826698303),\n",
       " ('first', 0.4025117754936218),\n",
       " ('flattest', 0.4017193019390106),\n",
       " ('consecutive', 0.3951870799064636)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.629394Z",
     "start_time": "2025-12-06T06:56:32.622595Z"
    }
   },
   "source": [
    "# more encyclopaedic knowledge\n",
    "news_model.most_similar(positive=['blue','tulip'], negative=['sky'], topn = 5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('purple', 0.5252774953842163),\n",
       " ('tulips', 0.4938238859176636),\n",
       " ('brown', 0.490774929523468),\n",
       " ('pink', 0.4860529899597168),\n",
       " ('maroon', 0.48056456446647644)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.663857Z",
     "start_time": "2025-12-06T06:56:32.646454Z"
    }
   },
   "source": [
    "# syntactic knowledge (pronouns)\n",
    "news_model.most_similar(positive=['him','she'], negative=['he'], topn = 5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('her', 0.804938554763794),\n",
       " ('herself', 0.6881043314933777),\n",
       " ('me', 0.5886672139167786),\n",
       " ('She', 0.5803765058517456),\n",
       " ('woman', 0.5470799207687378)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T06:56:32.712438Z",
     "start_time": "2025-12-06T06:56:32.702882Z"
    }
   },
   "source": [
    "# lexical know\n",
    "news_model.most_similar(positive=['light','long'], negative=['dark'], topn = 5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('short', 0.4077242910861969),\n",
       " ('longer', 0.3670077621936798),\n",
       " ('lengthy', 0.36229580640792847),\n",
       " ('Long', 0.3600355386734009),\n",
       " ('continuous', 0.34982720017433167)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Find the odd one out\n",
    "news_model.doesnt_match('breakfast cereal dinner lunch'.split())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz questions + homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model.most_similar(positive=['be','has'], negative=['is'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_model.wv.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_model.wv.most_similar('college', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model.most_similar('college', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model.similarity('university','turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model.similarity('university', 'school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model.similarity('university', 'factory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model.similarity('university', 'supermarket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model.similarity('university', 'turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
